{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paths\n",
    "import data\n",
    "import ctc_model\n",
    "import phoneme_list\n",
    "import UtteranceDataset\n",
    "import config\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_packages = [paths, data, ctc_model, util, config]\n",
    "for package in reload_packages:\n",
    "    importlib.reload(package)\n",
    "# importlib.reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24724,) (24724,)\n",
      "Dataset Device: cuda\n",
      "(1106,) (1106,)\n",
      "Dataset Device: cuda\n"
     ]
    }
   ],
   "source": [
    "train_loader = data.get_loader(\"train\")\n",
    "val_loader = data.get_loader(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ctc_model.SpeechModel(phoneme_list.N_PHONEMES,num_rnn=5,hidden_size=256,nlayers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechModel(\n",
      "  (rnns): ModuleList(\n",
      "    (0): LSTM(40, 256, bidirectional=True)\n",
      "    (1): LSTM(512, 256, bidirectional=True)\n",
      "    (2): LSTM(512, 256, bidirectional=True)\n",
      "    (3): LSTM(512, 256, bidirectional=True)\n",
      "    (4): LSTM(512, 256, bidirectional=True)\n",
      "  )\n",
      "  (scoring): Linear(in_features=512, out_features=47, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haomingli111/deeplearning/hw3/deep_learning_ctc/codes/ctc_model.py:325: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  bias_init(p.data)\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "ctc_model.init_weights(model, torch.nn.init.orthogonal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3*0.5, momentum=0.9, nesterov=True)\n",
    "\n",
    "best_epoch, best_vali_loss, starting_epoch = 0, 400, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.0005\n",
       "    momentum: 0.9\n",
       "    nesterov: True\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '../outputs/4bilstm_adam_10.pth.tar'\n",
      "=> loaded checkpoint '../outputs/4bilstm_adam_10.pth.tar' (epoch 10)\n"
     ]
    }
   ],
   "source": [
    "# proceeding from old models\n",
    "model_path = os.path.join(paths.output_path, '4bilstm_adam_10.pth.tar')\n",
    "print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "checkpoint = torch.load(model_path)\n",
    "starting_epoch = checkpoint['epoch']+1\n",
    "# best_vali_acc = checkpoint['best_vali_acc']\n",
    "model_state_dict = checkpoint['model_state_dict']\n",
    "model.load_state_dict(model_state_dict)\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_label_state_dict'])\n",
    "best_vali_loss = checkpoint['best_vali_loss']\n",
    "best_epoch = checkpoint['best_epoch']\n",
    "print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "      .format(model_path, checkpoint['epoch']))\n",
    "# del checkpoint, model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param_group in optimizer.param_groups:\n",
    "#     param_group['lr'] = lr=1e-3*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for state in optimizer.state.values():\n",
    "#     for k, v in state.items():\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             state[k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Starting training with scheduler.\n",
      "### Epoch    11 \n",
      "Epoch: 11\tBatch: 50\tAvg-Loss: 16.6724 \n",
      "Epoch: 11\tBatch: 100\tAvg-Loss: 16.2930 \n",
      "Epoch: 11\tBatch: 150\tAvg-Loss: 15.9613 \n",
      "Epoch: 11\tBatch: 200\tAvg-Loss: 15.6137 \n",
      "Epoch: 11\tBatch: 250\tAvg-Loss: 15.2193 \n",
      "Epoch: 11\tBatch: 300\tAvg-Loss: 16.1339 \n",
      "Epoch: 11\tBatch: 350\tAvg-Loss: 15.4541 \n",
      "Train Loss: 14.5812\tVal Loss: 24.5721\t \n",
      "start eval\n",
      "vali_distance: 7.0404\t \n",
      "Epoch time used:  818.1478321552277 s \n",
      "### Epoch    12 \n",
      "Epoch: 12\tBatch: 50\tAvg-Loss: 14.4943 \n",
      "Epoch: 12\tBatch: 100\tAvg-Loss: 14.0548 \n",
      "Epoch: 12\tBatch: 150\tAvg-Loss: 14.6938 \n",
      "Epoch: 12\tBatch: 200\tAvg-Loss: 15.3011 \n",
      "Epoch: 12\tBatch: 250\tAvg-Loss: 14.6722 \n",
      "Epoch: 12\tBatch: 300\tAvg-Loss: 13.8810 \n",
      "Train Loss: 13.5855\tVal Loss: 24.5578\t \n",
      "start eval\n",
      "vali_distance: 7.0009\t \n",
      "Epoch time used:  819.7332215309143 s \n",
      "### Epoch    13 \n",
      "Epoch: 13\tBatch: 50\tAvg-Loss: 13.3452 \n",
      "Epoch: 13\tBatch: 100\tAvg-Loss: 14.4191 \n",
      "Epoch: 13\tBatch: 150\tAvg-Loss: 13.2637 \n",
      "Epoch: 13\tBatch: 200\tAvg-Loss: 14.1425 \n",
      "Epoch: 13\tBatch: 250\tAvg-Loss: 14.0602 \n",
      "Epoch: 13\tBatch: 300\tAvg-Loss: 13.3693 \n",
      "Epoch: 13\tBatch: 350\tAvg-Loss: 14.0365 \n",
      "Train Loss: 13.5188\tVal Loss: 25.3743\t \n",
      "start eval\n",
      "vali_distance: 7.1213\t \n",
      "Epoch time used:  817.0532643795013 s \n",
      "### Epoch    14 \n",
      "Epoch: 14\tBatch: 50\tAvg-Loss: 13.4471 \n",
      "Epoch: 14\tBatch: 100\tAvg-Loss: 12.8911 \n",
      "Epoch: 14\tBatch: 150\tAvg-Loss: 12.9648 \n",
      "Epoch: 14\tBatch: 200\tAvg-Loss: 12.7308 \n",
      "Epoch: 14\tBatch: 250\tAvg-Loss: 13.0990 \n",
      "Epoch: 14\tBatch: 300\tAvg-Loss: 12.9643 \n",
      "Epoch: 14\tBatch: 350\tAvg-Loss: 14.1666 \n",
      "Train Loss: 12.5939\tVal Loss: 24.6132\t \n",
      "Epoch     3: reducing learning rate of group 0 to 2.5000e-04.\n",
      "start eval\n",
      "vali_distance: 6.9324\t \n",
      "Epoch time used:  814.8546788692474 s \n",
      "### Epoch    15 \n",
      "Epoch: 15\tBatch: 50\tAvg-Loss: 11.9433 \n",
      "Epoch: 15\tBatch: 100\tAvg-Loss: 12.1761 \n",
      "Epoch: 15\tBatch: 150\tAvg-Loss: 12.0969 \n",
      "Epoch: 15\tBatch: 200\tAvg-Loss: 13.4252 \n",
      "Epoch: 15\tBatch: 250\tAvg-Loss: 11.8494 \n",
      "Epoch: 15\tBatch: 300\tAvg-Loss: 12.1971 \n",
      "Epoch: 15\tBatch: 350\tAvg-Loss: 12.5011 \n",
      "Train Loss: 11.8246\tVal Loss: 24.3861\t \n",
      "start eval\n",
      "vali_distance: 6.7250\t \n",
      "Epoch time used:  817.5469658374786 s \n",
      "### Epoch    16 \n",
      "Epoch: 16\tBatch: 50\tAvg-Loss: 11.7488 \n",
      "Epoch: 16\tBatch: 100\tAvg-Loss: 12.4992 \n",
      "Epoch: 16\tBatch: 150\tAvg-Loss: 12.1220 \n",
      "Epoch: 16\tBatch: 200\tAvg-Loss: 12.5181 \n",
      "Epoch: 16\tBatch: 250\tAvg-Loss: 11.4731 \n",
      "Epoch: 16\tBatch: 300\tAvg-Loss: 12.3750 \n",
      "Epoch: 16\tBatch: 350\tAvg-Loss: 11.7413 \n",
      "Train Loss: 11.5890\tVal Loss: 24.6544\t \n",
      "start eval\n",
      "vali_distance: 6.8767\t \n",
      "Epoch time used:  821.424079656601 s \n",
      "### Epoch    17 \n",
      "Epoch: 17\tBatch: 50\tAvg-Loss: 11.2973 \n",
      "Epoch: 17\tBatch: 100\tAvg-Loss: 11.9624 \n",
      "Epoch: 17\tBatch: 150\tAvg-Loss: 12.1703 \n",
      "Epoch: 17\tBatch: 200\tAvg-Loss: 12.1592 \n",
      "Epoch: 17\tBatch: 250\tAvg-Loss: 11.2527 \n",
      "Epoch: 17\tBatch: 300\tAvg-Loss: 11.3654 \n",
      "Epoch: 17\tBatch: 350\tAvg-Loss: 11.6135 \n",
      "Train Loss: 11.3421\tVal Loss: 24.6819\t \n",
      "Epoch     6: reducing learning rate of group 0 to 1.2500e-04.\n",
      "start eval\n",
      "vali_distance: 6.8520\t \n",
      "Epoch time used:  820.0924003124237 s \n",
      "### Epoch    18 \n",
      "Epoch: 18\tBatch: 50\tAvg-Loss: 12.0341 \n",
      "Epoch: 18\tBatch: 100\tAvg-Loss: 11.0465 \n",
      "Epoch: 18\tBatch: 150\tAvg-Loss: 11.2493 \n",
      "Epoch: 18\tBatch: 200\tAvg-Loss: 11.4365 \n",
      "Epoch: 18\tBatch: 250\tAvg-Loss: 10.9443 \n",
      "Epoch: 18\tBatch: 300\tAvg-Loss: 10.9315 \n",
      "Epoch: 18\tBatch: 350\tAvg-Loss: 11.0587 \n",
      "Train Loss: 11.0652\tVal Loss: 24.6110\t \n",
      "start eval\n",
      "vali_distance: 6.7518\t \n",
      "Epoch time used:  816.4795274734497 s \n",
      "### Epoch    19 \n",
      "Epoch: 19\tBatch: 50\tAvg-Loss: 10.3237 \n",
      "Epoch: 19\tBatch: 100\tAvg-Loss: 11.0131 \n",
      "Epoch: 19\tBatch: 150\tAvg-Loss: 11.5434 \n",
      "Epoch: 19\tBatch: 200\tAvg-Loss: 11.8136 \n",
      "Epoch: 19\tBatch: 250\tAvg-Loss: 10.8822 \n",
      "Epoch: 19\tBatch: 300\tAvg-Loss: 11.5197 \n",
      "Epoch: 19\tBatch: 350\tAvg-Loss: 10.9353 \n",
      "Train Loss: 10.8751\tVal Loss: 24.7670\t \n",
      "Epoch     8: reducing learning rate of group 0 to 6.2500e-05.\n",
      "start eval\n",
      "vali_distance: 6.7908\t \n",
      "Epoch time used:  817.000492811203 s \n",
      "### Epoch    20 \n",
      "Epoch: 20\tBatch: 50\tAvg-Loss: 10.5583 \n",
      "Epoch: 20\tBatch: 100\tAvg-Loss: 10.6025 \n",
      "Epoch: 20\tBatch: 150\tAvg-Loss: 10.7683 \n",
      "Epoch: 20\tBatch: 200\tAvg-Loss: 11.0351 \n",
      "Epoch: 20\tBatch: 250\tAvg-Loss: 10.2307 \n",
      "Epoch: 20\tBatch: 300\tAvg-Loss: 12.3541 \n",
      "Epoch: 20\tBatch: 350\tAvg-Loss: 10.1795 \n",
      "Train Loss: 10.7456\tVal Loss: 24.6417\t \n",
      "start eval\n",
      "vali_distance: 6.7536\t \n",
      "Epoch time used:  820.475127696991 s \n",
      "### Epoch    21 \n",
      "Epoch: 21\tBatch: 50\tAvg-Loss: 10.8202 \n",
      "Epoch: 21\tBatch: 100\tAvg-Loss: 9.9365 \n",
      "Epoch: 21\tBatch: 150\tAvg-Loss: 11.4890 \n",
      "Epoch: 21\tBatch: 200\tAvg-Loss: 11.0022 \n",
      "Epoch: 21\tBatch: 250\tAvg-Loss: 10.7100 \n",
      "Epoch: 21\tBatch: 300\tAvg-Loss: 10.7831 \n",
      "Epoch: 21\tBatch: 350\tAvg-Loss: 11.1668 \n",
      "Train Loss: 10.6710\tVal Loss: 24.7646\t \n",
      "Epoch    10: reducing learning rate of group 0 to 3.1250e-05.\n",
      "start eval\n",
      "vali_distance: 6.7733\t \n",
      "Epoch time used:  818.6099138259888 s \n",
      "### Epoch    22 \n",
      "Epoch: 22\tBatch: 50\tAvg-Loss: 10.8389 \n",
      "Epoch: 22\tBatch: 100\tAvg-Loss: 10.5820 \n",
      "Epoch: 22\tBatch: 150\tAvg-Loss: 11.4871 \n",
      "Epoch: 22\tBatch: 200\tAvg-Loss: 10.9709 \n",
      "Epoch: 22\tBatch: 250\tAvg-Loss: 10.1553 \n",
      "Epoch: 22\tBatch: 300\tAvg-Loss: 10.3899 \n",
      "Epoch: 22\tBatch: 350\tAvg-Loss: 10.9459 \n",
      "Train Loss: 10.6601\tVal Loss: 24.7716\t \n",
      "start eval\n",
      "vali_distance: 6.7841\t \n",
      "Epoch time used:  817.5678598880768 s \n",
      "### Epoch    23 \n",
      "Epoch: 23\tBatch: 50\tAvg-Loss: 11.0156 \n",
      "Epoch: 23\tBatch: 100\tAvg-Loss: 9.7267 \n",
      "Epoch: 23\tBatch: 150\tAvg-Loss: 11.1225 \n",
      "Epoch: 23\tBatch: 200\tAvg-Loss: 10.2279 \n",
      "Epoch: 23\tBatch: 250\tAvg-Loss: 10.4689 \n",
      "Epoch: 23\tBatch: 300\tAvg-Loss: 11.3924 \n",
      "Epoch: 23\tBatch: 350\tAvg-Loss: 10.4827 \n",
      "Train Loss: 10.5622\tVal Loss: 24.8049\t \n",
      "Epoch    12: reducing learning rate of group 0 to 1.5625e-05.\n",
      "start eval\n",
      "vali_distance: 6.7732\t \n",
      "Epoch time used:  816.7681214809418 s \n",
      "### Epoch    24 \n",
      "Epoch: 24\tBatch: 50\tAvg-Loss: 10.3619 \n",
      "Epoch: 24\tBatch: 100\tAvg-Loss: 10.4903 \n",
      "Epoch: 24\tBatch: 150\tAvg-Loss: 10.5000 \n",
      "Epoch: 24\tBatch: 200\tAvg-Loss: 10.8632 \n",
      "Epoch: 24\tBatch: 250\tAvg-Loss: 10.5014 \n",
      "Epoch: 24\tBatch: 300\tAvg-Loss: 9.5652 \n",
      "Epoch: 24\tBatch: 350\tAvg-Loss: 11.3132 \n",
      "Train Loss: 10.5364\tVal Loss: 24.8095\t \n",
      "start eval\n",
      "vali_distance: 6.7435\t \n",
      "Epoch time used:  819.6377060413361 s \n",
      "### Epoch    25 \n",
      "Epoch: 25\tBatch: 50\tAvg-Loss: 10.7831 \n",
      "Epoch: 25\tBatch: 100\tAvg-Loss: 10.0387 \n",
      "Epoch: 25\tBatch: 150\tAvg-Loss: 10.3015 \n",
      "Epoch: 25\tBatch: 200\tAvg-Loss: 10.1800 \n",
      "Epoch: 25\tBatch: 250\tAvg-Loss: 9.7390 \n",
      "Epoch: 25\tBatch: 300\tAvg-Loss: 11.6034 \n",
      "Epoch: 25\tBatch: 350\tAvg-Loss: 11.3429 \n",
      "Train Loss: 10.5105\tVal Loss: 24.8046\t \n",
      "Epoch    14: reducing learning rate of group 0 to 7.8125e-06.\n",
      "start eval\n",
      "vali_distance: 6.7567\t \n",
      "Epoch time used:  821.0256237983704 s \n",
      "Summary: \n",
      "- Best Epoch: 15 | - Best Val Acc: 24 \n"
     ]
    }
   ],
   "source": [
    "# ctc_model.run(model, optimizer, train_loader, val_loader, best_epoch, best_vali_loss, starting_epoch)\n",
    "ctc_model.run_with_scheduler(model,\n",
    "                             optimizer, 1, 0.5,\n",
    "                             train_loader, val_loader, \n",
    "                             best_epoch, best_vali_loss, starting_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.wrap_up_experiment(os.path.join(paths.output_path, 'metrics.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctc_model.run_eval(model_2, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Device: cuda\n"
     ]
    }
   ],
   "source": [
    "test_loader = data.get_loader(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '../outputs/4bilstm_adam_15.pth.tar'\n",
      "=> loaded checkpoint '../outputs/4bilstm_adam_15.pth.tar' (epoch 15)\n",
      "start prediction\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "=> loading checkpoint '../outputs/4bilstm_adam_25.pth.tar'\n",
      "=> loaded checkpoint '../outputs/4bilstm_adam_25.pth.tar' (epoch 25)\n",
      "start prediction\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "for epoch in [15, 25]:\n",
    "    # checkpoint = torch.load(\"checkpoint.pt\")\n",
    "    model_prediction = ctc_model.SpeechModel(phoneme_list.N_PHONEMES,num_rnn=5,hidden_size=256,nlayers=1)\n",
    "\n",
    "    # proceeding from old models\n",
    "    model_path = os.path.join(paths.output_path, '4bilstm_adam_'+str(epoch)+'.pth.tar')\n",
    "    print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "    checkpoint = torch.load(model_path)\n",
    "    starting_epoch = checkpoint['epoch']\n",
    "    # best_vali_acc = checkpoint['best_vali_acc']\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    model_prediction.load_state_dict(model_state_dict)\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    best_vali_loss = checkpoint['best_vali_loss']\n",
    "    best_epoch = checkpoint['best_epoch']\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(model_path, checkpoint['epoch']))\n",
    "    # del checkpoint, model_state_dict\n",
    "\n",
    "    best_epoch\n",
    "\n",
    "    model_prediction.cuda()\n",
    "\n",
    "    encoded_prediction = ctc_model.predict(model_prediction, test_loader)\n",
    "\n",
    "    import pandas as pd\n",
    "    varification_results_df = pd.DataFrame({'Id':np.arange(len(test_loader)), 'predicted':np.array(encoded_prediction).flatten()})\n",
    "\n",
    "    varification_results_df.to_csv(os.path.join('..','outputs', 'verification_submission_'+str(epoch)+'.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for data in test_loader:\n",
    "    test = data\n",
    "    print(len([data]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0130, -4.4909, -4.8291,  ..., -2.0473, -3.3245, -2.6745],\n",
       "        [-4.0408, -6.3393, -5.9088,  ..., -1.2934, -2.3482, -3.8307],\n",
       "        [-6.5768, -8.1930, -4.9924,  ..., -1.7163, -2.9462, -3.5497],\n",
       "        ...,\n",
       "        [-7.2773, -2.9609, -3.3077,  ..., -3.0296, -1.9444, -2.8614],\n",
       "        [-5.2828, -3.2241, -4.2793,  ..., -1.0198, -2.6685, -4.1456],\n",
       "        [-4.9332, -4.8754, -4.3110,  ..., -0.7746, -2.8968, -3.4207]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 7.7970, -1.3460,  0.0933,  ..., -5.0684, -4.3781, -5.3427]],\n",
       " \n",
       "         [[ 9.0795, -1.6963, -0.7084,  ..., -6.0590, -4.3441, -6.3822]],\n",
       " \n",
       "         [[ 9.9727, -1.9067, -1.3714,  ..., -6.1406, -4.4554, -7.0818]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 5.9954,  0.4422, -0.6152,  ..., -2.7546, -0.4327, -5.7047]],\n",
       " \n",
       "         [[ 5.3743, -0.7836, -0.9460,  ..., -1.6021, -0.8774, -4.4772]],\n",
       " \n",
       "         [[ 4.6321, -1.7703, -1.2969,  ..., -1.3024, -2.1310, -4.3121]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>), tensor([542]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
